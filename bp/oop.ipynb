{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "boxed-scanner",
   "metadata": {},
   "source": [
    "### Implementation of backpropagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "equipped-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from numpy import *\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-vienna",
   "metadata": {},
   "source": [
    "### 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-dominican",
   "metadata": {},
   "source": [
    "$$sigmoid(x) = \\frac{1}{1+\\exp{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "technological-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    激励函数\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return 1.0 / (1 + exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-freedom",
   "metadata": {},
   "source": [
    "### 输出值\n",
    "设$net_j=\\vec{w_j} \\cdot \\vec{x_j}=\\sum_iw_{ji}x_{ji}$\n",
    "\n",
    "对于输入层\n",
    "$$\n",
    "a_1=f(net_1)\n",
    "$$\n",
    "对于隐藏层\n",
    "$$\n",
    "a_k=f(net_k)\n",
    "$$\n",
    "对于输出层}\n",
    "$$\n",
    "y=f(net_n)\n",
    "$$\n",
    "其中$f(x)$在这个实验中为$sigmoid(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "advised-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_output(self):\n",
    "    \"\"\"\n",
    "    计算当前节点的输出\n",
    "    \"\"\"\n",
    "    output = reduce(lambda ret, conn: ret + conn.upstream_node.output * conn.weight, self.upstream, 0)\n",
    "    # 结果放入激活函数\n",
    "    self.output = sigmoid(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-kingdom",
   "metadata": {},
   "source": [
    "### 误差项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-viking",
   "metadata": {},
   "source": [
    "#### 对于输出层\n",
    "$$\n",
    "\\delta_j=(t_j-y_j)y_j(1-y_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "excess-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_output_layer_delta(self, label):\n",
    "    \"\"\"\n",
    "    计算输出层的误差项\n",
    "    :param label: 样本的目标值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    self.delta = self.output * (1 - self.output) * (label - self.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-demand",
   "metadata": {},
   "source": [
    "#### 对于隐藏层\n",
    "$$\n",
    "\\delta_j=\\alpha_j(1-\\alpha_j)\\sum_{k \\in Downstream}\\delta_kw_{kj}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "angry-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hidden_layer_delta(self):\n",
    "    \"\"\"\n",
    "    计算隐藏层的误差项\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    downstream_delta = reduce(\n",
    "        lambda ret, conn: ret + conn.downstream_node.delta * conn.weight,\n",
    "        self.downstream, 0.0)\n",
    "    self.delta = self.output * (1 - self.output) * downstream_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-constitution",
   "metadata": {},
   "source": [
    "### 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-sender",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "w_{ji}&=w_{ji}-\\mu\\frac{\\partial E_d}{\\partial w_{ji}} \\\\\n",
    "&=w_{ji}+\\mu \\delta_j x_{ji}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "plain-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient(self):\n",
    "    self.gradient = self.downstream_node.delta * self.upstream_node.output\n",
    "\n",
    "def update_weight(self, rate):\n",
    "    \"\"\"\n",
    "    :param rate: 学习率\n",
    "    \"\"\"\n",
    "    self.calc_gradient()\n",
    "    self.weight += rate * self.gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-garbage",
   "metadata": {},
   "source": [
    "### 总代吗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "funded-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\n",
    "import random\n",
    "from numpy import *\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    激励函数\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return 1.0 / (1 + exp(-x))\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, layer_index, node_index):\n",
    "        \"\"\"\n",
    "        :param layer_index: 层编号\n",
    "        :param node_index: 节点编号\n",
    "        \"\"\"\n",
    "        self.layer_index = layer_index\n",
    "        self.node_index = node_index\n",
    "        self.downstream = []  # 下游节点\n",
    "        self.upstream = []  # 上游节点\n",
    "        self.output = 0  # 输出值\n",
    "        self.delta = 0  # 误差项\n",
    "\n",
    "    def set_output(self, output):\n",
    "        \"\"\"\n",
    "        设置节点的输出，一般用在输入层节点，输入数据\n",
    "        :param output: 赋予输出节点的值，长度等于该层除了偏置节点的数量。\n",
    "        \"\"\"\n",
    "        self.output = output\n",
    "\n",
    "    def append_downstream_connection(self, conn):\n",
    "        \"\"\"\n",
    "        当前节点连接下游节点的边\n",
    "        :param conn:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.downstream.append(conn)\n",
    "\n",
    "    def append_upstream_connection(self, conn):\n",
    "        \"\"\"\n",
    "        当前节点连接上游节点的边\n",
    "        :param conn:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.upstream.append(conn)\n",
    "\n",
    "    def calc_output(self):\n",
    "        \"\"\"\n",
    "        计算当前节点的输出\n",
    "        netj = sum wji * xji\n",
    "        alphaj = sigmoid(netj)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = reduce(lambda ret, conn: ret + conn.upstream_node.output * conn.weight, self.upstream, 0)\n",
    "        # 结果放入激活函数\n",
    "        self.output = sigmoid(output)\n",
    "\n",
    "    def calc_hidden_layer_delta(self):\n",
    "        \"\"\"\n",
    "        计算隐藏层的误差项\n",
    "        deltaj=alphaj(1-alphaj) sum deltak*wkj\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        downstream_delta = reduce(\n",
    "            lambda ret, conn: ret + conn.downstream_node.delta * conn.weight,\n",
    "            self.downstream, 0.0)\n",
    "        self.delta = self.output * (1 - self.output) * downstream_delta\n",
    "\n",
    "    def calc_output_layer_delta(self, label):\n",
    "        \"\"\"\n",
    "        计算输出层的误差项\n",
    "        :param label:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.delta = self.output * (1 - self.output) * (label - self.output)\n",
    "\n",
    "    def __str__(self):\n",
    "        node_str = '%u-%u: output: %f delta: %f' % (self.layer_index, self.node_index, self.output, self.delta)\n",
    "        downstream_str = reduce(lambda ret, conn: ret + '\\n\\t' + str(conn), self.downstream, '')\n",
    "        upstream_str = reduce(lambda ret, conn: ret + '\\n\\t' + str(conn), self.upstream, '')\n",
    "        return node_str + '\\n\\tdownstream:' + downstream_str + '\\n\\tupstream:' + upstream_str\n",
    "\n",
    "\n",
    "class ConstNode(object):\n",
    "    def __init__(self, layer_index, node_index):\n",
    "        self.layer_index = layer_index\n",
    "        self.node_index = node_index\n",
    "        self.downstream = []\n",
    "        self.output = 1\n",
    "\n",
    "    def append_downstream_connection(self, conn):\n",
    "        self.downstream.append(conn)\n",
    "\n",
    "    def calc_hidden_layer_delta(self):\n",
    "        downstream_delta = reduce(\n",
    "            lambda ret, conn: ret + conn.downstream_node.delta * conn.weight,\n",
    "            self.downstream, 0.0)\n",
    "        self.delta = self.output * (1 - self.output) * downstream_delta\n",
    "\n",
    "    def __str__(self):\n",
    "        node_str = '%u-%u: output: 1' % (self.layer_index, self.node_index)\n",
    "        downstream_str = reduce(lambda ret, conn: ret + '\\n\\t' + str(conn), self.downstream, '')\n",
    "        return node_str + '\\n\\tdownstream:' + downstream_str\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, layer_index, node_count):\n",
    "        self.layer_index = layer_index\n",
    "        self.nodes = []\n",
    "        # 初始化节点对象\n",
    "        for i in range(node_count):\n",
    "            self.nodes.append(Node(layer_index, i))\n",
    "        self.nodes.append(ConstNode(layer_index, node_count))\n",
    "\n",
    "    def set_output(self, data):\n",
    "        for i in range(len(data)):\n",
    "            self.nodes[i].set_output(data[i])\n",
    "\n",
    "    def calc_output(self):\n",
    "        for node in self.nodes[:-1]:\n",
    "            node.calc_output()\n",
    "\n",
    "    def dump(self):\n",
    "        for node in self.nodes:\n",
    "            print(node)\n",
    "\n",
    "\n",
    "class Connection(object):\n",
    "    def __init__(self, upstream_node, downstream_node):\n",
    "        self.upstream_node = upstream_node\n",
    "        self.downstream_node = downstream_node\n",
    "        self.weight = random.uniform(-0.1, 0.1)\n",
    "        self.gradient = 0.0\n",
    "\n",
    "    def calc_gradient(self):\n",
    "        self.gradient = self.downstream_node.delta * self.upstream_node.output\n",
    "\n",
    "    def update_weight(self, rate):\n",
    "        self.calc_gradient()\n",
    "        self.weight += rate * self.gradient\n",
    "\n",
    "    def get_gradient(self):\n",
    "        return self.gradient\n",
    "\n",
    "    def __str__(self):\n",
    "        return '(%u-%u) -> (%u-%u) = %f' % (\n",
    "            self.upstream_node.layer_index,\n",
    "            self.upstream_node.node_index,\n",
    "            self.downstream_node.layer_index,\n",
    "            self.downstream_node.node_index,\n",
    "            self.weight)\n",
    "\n",
    "\n",
    "class Connections(object):\n",
    "    def __init__(self):\n",
    "        self.connections = []\n",
    "\n",
    "    def add_connection(self, connection):\n",
    "        self.connections.append(connection)\n",
    "\n",
    "    def dump(self):\n",
    "        for conn in self.connections:\n",
    "            print(conn)\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, layers):\n",
    "        self.connections = Connections()\n",
    "        self.layers = []\n",
    "        # 计算网络层数\n",
    "        layer_count = len(layers)\n",
    "        node_count = 0\n",
    "        # 初始化网络层，网错层对象append在self.layers 里面，而节点对象又在layer里面被初始化\n",
    "        # Connections 仅仅作为Connection的集合对象，提供一些集合操作, 而layer有是节点对象合集\n",
    "        for i in range(layer_count):\n",
    "            self.layers.append(Layer(i, layers[i]))\n",
    "        for layer in range(layer_count - 1):\n",
    "            connections = [Connection(upstream_node, downstream_node)\n",
    "                           for upstream_node in self.layers[layer].nodes\n",
    "                           for downstream_node in self.layers[layer + 1].nodes[:-1]]\n",
    "            for conn in connections:\n",
    "                self.connections.add_connection(conn)\n",
    "                conn.downstream_node.append_upstream_connection(conn)\n",
    "                conn.upstream_node.append_downstream_connection(conn)\n",
    "\n",
    "    def train(self, labels, data_set, rate, epoch):\n",
    "        for i in range(epoch):\n",
    "            for d in range(len(data_set)):\n",
    "                self.train_one_sample(labels[d], data_set[d], rate)\n",
    "                # print 'sample %d training finished' % d\n",
    "\n",
    "    def train_one_sample(self, label, sample, rate):\n",
    "        self.predict(sample)\n",
    "        self.calc_delta(label)\n",
    "        self.update_weight(rate)\n",
    "\n",
    "    def calc_delta(self, label):\n",
    "        output_nodes = self.layers[-1].nodes\n",
    "        for i in range(len(label)):\n",
    "            output_nodes[i].calc_output_layer_delta(label[i])\n",
    "        for layer in self.layers[-2::-1]:\n",
    "            for node in layer.nodes:\n",
    "                node.calc_hidden_layer_delta()\n",
    "\n",
    "    def update_weight(self, rate):\n",
    "        for layer in self.layers[:-1]:\n",
    "            for node in layer.nodes:\n",
    "                for conn in node.downstream:\n",
    "                    conn.update_weight(rate)\n",
    "\n",
    "    def calc_gradient(self):\n",
    "        for layer in self.layers[:-1]:\n",
    "            for node in layer.nodes:\n",
    "                for conn in node.downstream:\n",
    "                    conn.calc_gradient()\n",
    "\n",
    "    def get_gradient(self, label, sample):\n",
    "        self.predict(sample)\n",
    "        self.calc_delta(label)\n",
    "        self.calc_gradient()\n",
    "\n",
    "    def predict(self, sample):\n",
    "        self.layers[0].set_output(sample)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].calc_output()\n",
    "        return list(map(lambda node: node.output, self.layers[-1].nodes[:-1]))\n",
    "\n",
    "    def dump(self):\n",
    "        for layer in self.layers:\n",
    "            layer.dump()\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "    def __init__(self):\n",
    "        self.mask = [\n",
    "            0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, 0x80\n",
    "        ]\n",
    "\n",
    "    def norm(self, number):\n",
    "        return list(map(lambda m: 0.9 if number & m else 0.1, self.mask))\n",
    "\n",
    "    def denorm(self, vec):\n",
    "        binary = list(map(lambda i: 1 if i > 0.5 else 0, vec))\n",
    "        for i in range(len(self.mask)):\n",
    "            binary[i] = binary[i] * self.mask[i]\n",
    "        return reduce(lambda x, y: x + y, binary)\n",
    "\n",
    "\n",
    "def mean_square_error(vec1, vec2):\n",
    "    return 0.5 * reduce(lambda a, b: a + b,\n",
    "                        list(map(lambda v: (v[0] - v[1]) * (v[0] - v[1]),\n",
    "                                 zip(vec1, vec2)\n",
    "                                 ))\n",
    "                        )\n",
    "\n",
    "\n",
    "def gradient_check(network, sample_feature, sample_label):\n",
    "    '''\n",
    "    梯度检查\n",
    "    network: 神经网络对象\n",
    "    sample_feature: 样本的特征\n",
    "    sample_label: 样本的标签\n",
    "    '''\n",
    "    # 计算网络误差\n",
    "    network_error = lambda vec1, vec2: \\\n",
    "        0.5 * reduce(lambda a, b: a + b,\n",
    "                     list(map(lambda v: (v[0] - v[1]) * (v[0] - v[1]),\n",
    "                              zip(vec1, vec2))))\n",
    "\n",
    "    # 获取网络在当前样本下每个连接的梯度\n",
    "    network.get_gradient(sample_feature, sample_label)\n",
    "\n",
    "    # 对每个权重做梯度检查    \n",
    "    for conn in network.connections.connections:\n",
    "        # 获取指定连接的梯度\n",
    "        actual_gradient = conn.get_gradient()\n",
    "\n",
    "        # 增加一个很小的值，计算网络的误差\n",
    "        epsilon = 0.0001\n",
    "        conn.weight += epsilon\n",
    "        error1 = network_error(network.predict(sample_feature), sample_label)\n",
    "\n",
    "        # 减去一个很小的值，计算网络的误差\n",
    "        conn.weight -= 2 * epsilon  # 刚才加过了一次，因此这里需要减去2倍\n",
    "        error2 = network_error(network.predict(sample_feature), sample_label)\n",
    "\n",
    "        # 根据式6计算期望的梯度值\n",
    "        expected_gradient = (error2 - error1) / (2 * epsilon)\n",
    "\n",
    "        # 打印\n",
    "        print('expected gradient: \\t%f\\nactual gradient: \\t%f' % (\n",
    "            expected_gradient, actual_gradient))\n",
    "\n",
    "\n",
    "def train_data_set():\n",
    "    normalizer = Normalizer()\n",
    "    data_set = []\n",
    "    labels = []\n",
    "    for i in range(0, 256, 8):\n",
    "        n = normalizer.norm(int(random.uniform(0, 256)))\n",
    "        data_set.append(n)\n",
    "        labels.append(n)\n",
    "    return labels, data_set\n",
    "\n",
    "\n",
    "def train(network):\n",
    "    assert isinstance(network, object)\n",
    "    labels, data_set = train_data_set()\n",
    "    print(labels)\n",
    "    network.train(labels, data_set, 0.3, 50)\n",
    "\n",
    "\n",
    "def test(network, data):\n",
    "    normalizer = Normalizer()\n",
    "    norm_data = normalizer.norm(data)\n",
    "    predict_data = network.predict(norm_data)\n",
    "    print('\\ttestdata(%u)\\tpredict(%u)' % (\n",
    "        data, normalizer.denorm(predict_data)))\n",
    "\n",
    "\n",
    "def correct_ratio(network):\n",
    "    normalizer = Normalizer()\n",
    "    correct = 0.0\n",
    "    for i in range(256):\n",
    "        if normalizer.denorm(network.predict(normalizer.norm(i))) == i:\n",
    "            correct += 1.0\n",
    "    print('correct_ratio: %.2f%%' % (correct / 256 * 100))\n",
    "\n",
    "\n",
    "def gradient_check_test():\n",
    "    net = Network([2, 2, 2])\n",
    "    sample_feature = [0.9, 0.1]\n",
    "    sample_label = [0.9, 0.1]\n",
    "    gradient_check(net, sample_feature, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-sympathy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
